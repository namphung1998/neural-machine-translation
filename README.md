# Neural Machine Translation with RNNs

The sequence-to-sequence (seq2seq) architecture has achieved significant results in various NLP tasks. This is an implementation of the seq2seq architecture with attention mechanism as detailed in Bahdanau et. al (2015). The model is implemented using the deep learning framework PyTorch. 

### Author: Nam Phung